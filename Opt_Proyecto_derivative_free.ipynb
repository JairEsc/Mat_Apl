{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled108.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpN8WZsY0kHQlyr76Cp9AI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JairEsc/Mat_Apl/blob/main/Opt_Proyecto_derivative_free.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.optimize\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "f_eEzJF9qQT8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def L_BFGS(S,Y,g):#Checar las posibles divisiones por cero\n",
        "    n=len(g)\n",
        "    m=len(S)\n",
        "    #supondremos H_0 un multiplo de la identidad\n",
        "    H_0=(np.dot(S[-1],Y[-1])/(np.dot(Y[-1],Y[-1])))*np.identity(n)#Dado en clase.\n",
        "    q=g\n",
        "    alphas=[]\n",
        "    for i in range(m):\n",
        "        alphas.append((1/max(10**(-6),np.dot(S[m-1-i],Y[m-1-i])))*np.dot(S[m-1-i],q))\n",
        "        q=q-alphas[i]*Y[m-1-i]\n",
        "    r=np.dot(H_0,q)\n",
        "    for i in range(m):\n",
        "        beta=(1/max(10**(-6),np.dot(Y[i],S[i])))*np.dot(Y[i],r)\n",
        "        r=r+S[i]*(alphas[m-1-i]-beta)#aproximacion de -H*g\n",
        "    return r"
      ],
      "metadata": {
        "id": "8zB4uyJdLJ6g"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idea general del algoritmo.\n",
        "\n",
        "Input: \n",
        "\n",
        "\n",
        "*   $f:\\mathbb{R}^n⟶ \\mathbb{R}$, función objetivo.\n",
        "*   $x_0$, aproximación inicial.\n",
        "*   $a_{max}$, Número máximo de iteraciones para la busqueda en linea.\n",
        "*   $\\zeta$, parámetro de curvatura.\n",
        "*   $tol_g$, criterio de paro para la norma del gradiente.\n",
        "\n",
        "\n",
        "\n",
        "Descripción general del algoritmo (Ver Derivative-free optimization, pag 9.)\n",
        "\n",
        "*   Calcular $f(x_0)$\n",
        "*   Estimar $\\epsilon_f$ usando ECnoise*\n",
        "*   Calcular ** h\n",
        "*   Calcular $\\nabla_h f(x_0)$. Cuando se haga, guardar los valores $(x_s,f_s)$ que satisfacen $f_s=f(x_s)=min_{x\\in S} f(x)$, $S=\\{x+h\\cdot e_i,i=1,\\ldots,n\\}$\n",
        "*   While ($||\\nabla f(x_k)||>tol_g$):\n",
        "    *   Calcular $d_k=-H_k\\nabla_h f(x_k)$ usando L-BFGS***\n",
        "    *   Hacer line_search$(x_k,f_k,\\nabla_h f(x_k),d_k,a_{max})$ y obtiene $(x_+,f_+,\\alpha_k,LS_{flag})$\n",
        "        *   If($LS_{flag}==1$):\n",
        "            *   Invocar Recovery($x_s,f_s$)\n",
        "        *   Else:\n",
        "            *   Actualizar $x_{k+1}=x_+,$ $f_{k+1}=f_+$\n",
        "    *   Calcular $\\nabla_h f(x_{k+1})$ y guardar $(x_s,f_s)$\n",
        "    *   Calcular $s_k=x_{k+1}-x_k$, $y_k=\\nabla_h f(x_{k+1})-\\nabla_h f(x_k)$\n",
        "        *   Guardarlos si $s_k^Ty_k\\geq \\zeta||s_k||||y_k||$"
      ],
      "metadata": {
        "id": "1st1PXXNxrWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación de L-BFGS: (Ver clase en Moodle.)\n",
        "\n",
        "Si $H_k$ es una aproximación de la inversa del Hessiano, nos interesa caluclar $d_k=-H_k\\nabla f(x_k)$.\n",
        "\n",
        "\n",
        "Input:\n",
        "\n",
        "\n",
        "\n",
        "*   $\\nabla f(x_k)$\n",
        "*   $\\{(s_{k-1},y_{k-1}),(s_{k-2},y_{k-2}),\\ldots,(s_{k-m},y_{k-m})\\}$\n",
        "*   $H^0_{k-m}$\n",
        "\n",
        "Algoritmo:\n",
        "\n",
        "\n",
        "*   Define $q=\\nabla f(x_k)$\n",
        "*   for $i=1,2,\\ldots,m$ do:\n",
        "    *   Calcular $\\alpha_i=\\rho_i s_{k-i}^Tq$\n",
        "    *   Actualiza $q=q-\\alpha_iy_i$\n",
        "*   Define $r=H^0_{k-m}q$\n",
        "*   for $i=1,2,\\ldots,m$ do:\n",
        "    *   Calcular $\\beta=\\rho_{k-i}y_{k-i}^Tr$\n",
        "    *   Actualiza $r=r-s_{k-i}(\\alpha_i-\\beta)$\n",
        "*   $r$ es una aproximación de $H_k\\nabla f(x_k)$\n"
      ],
      "metadata": {
        "id": "o9e9CVolHhpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ECNoise(F):#F es un vector que guarda f(t_i), i=0,...,m. Evaluaciones de m+1 puntos equiespaciados\n",
        "#Ver metodo en Estimating_Computational_Noise, pagina 8\n",
        "    m=len(F)-1\n",
        "    T=np.zeros((m+1,m+1))\n",
        "    for i in range(m+1):\n",
        "        T[i,0]=F[i]\n",
        "    for k in range(m):\n",
        "        for i in range(m-k):\n",
        "            T[i,k+1]=T[i+1,k]-T[i,k]\n",
        "    return T"
      ],
      "metadata": {
        "id": "EiaLoRSdNFgA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def var_estimation(T):#Recibe la tabla.\n",
        "    m=len(T)-1\n",
        "    sigma_array=[]\n",
        "    for k in range(1,m):\n",
        "        sigma_array.append(((np.math.factorial(k)/np.math.factorial(2*k))/(m+1-k))*np.sum(T.T[k]**2))\n",
        "    #Ahora checamos las 2 condiciones.\n",
        "    for k in range(1,m-2):\n",
        "        max_k=np.max(sigma_array[k:k+2])\n",
        "        min_k=np.min(sigma_array[k:k+2])\n",
        "        if(max_k<=4*min_k):\n",
        "            if(np.sign(min_k*max_k)==-1):\n",
        "                print(\"es el \", k)\n",
        "                break\n",
        "    return np.sqrt(sigma_array[k])#Podria ser que las condiciones no se cumplan, en tal caso regresa la ultima estimacion sigma."
      ],
      "metadata": {
        "id": "gQcG9CUNagT4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aprox_f_biprima(e_f,f,x_k,p):#Usando una direccion (aleatoria dada) se aproxima el max ||Hessiana_f(x_k)*p||\n",
        "    #Ver Estimating Derivatives of Noisy Simulations, pagina 14.\n",
        "    tao_1=100\n",
        "    tao_2=0.1\n",
        "    h_a=e_f**(1/4)\n",
        "    f_mas=f(x_k+h_a*p)\n",
        "    f_menos=f(x_k-h_a*p)\n",
        "    f_0=f(x_k)\n",
        "    delta_h_a=abs(f_mas+f_menos-2*f_0)\n",
        "    mu_a=delta_h_a/(h_a**2)\n",
        "    if(delta_h_a/e_f>=tao_1):\n",
        "        mu=mu_a\n",
        "        if(abs(f_mas-f_0)<=tao_2*max(f_mas,f_menos,f_0) and abs(f_menos-f_0)<=tao_2*max(f_mas,f_menos,f_0)):\n",
        "            return mu_a\n",
        "    h_b=(e_f/mu_a)**(1/4)\n",
        "    f_mas=f(x_k+h_b*p)\n",
        "    f_menos=f(x_k-h_b*p)\n",
        "    delta_h_b=abs(f_mas+f_menos-2*f_0)\n",
        "    mu_b=delta_h_b/(h_b**2)\n",
        "    if(delta_h_b/e_f>=tao_1):\n",
        "        mu=mu_b\n",
        "        if(abs(f_mas-f_0)<=tao_2*max(f_mas,f_menos,f_0) and abs(f_menos-f_0)<=tao_2*max(f_mas,f_menos,f_0)):\n",
        "            return mu_b\n",
        "    if(abs(mu_a-mu_b)<=0.5*mu_b):\n",
        "        return mu_b\n",
        "    print(\"No hay regla de decision\")\n",
        "    return mu_b#Por falta de relga de descicion. En el algoritmo no se especifica una opcion.\n"
      ],
      "metadata": {
        "id": "Y9wkwLTs-3Sh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rutina_h(f,p,x_0,m=6):#Calcula h a partir de una direccion dada, con m+1 puntos equiespaciados.\n",
        "    f_0=f(x_0)\n",
        "    n=len(x_0)\n",
        "    delta=10**(-3)#Suggested by ECnoise algorithm.\n",
        "    F=[f(x_0+delta*i*p) for i in range(m)]\n",
        "    T=ECNoise(F)\n",
        "    e_f=var_estimation(T)\n",
        "    #print(\"e_f: \",e_f)#Breakpoint\n",
        "    mu=aprox_f_biprima(e_f=e_f,f=f,x_k=x_0,p=p)\n",
        "    h=(8**(1/4))*(e_f/mu)**(1/2)\n",
        "    return e_f,h"
      ],
      "metadata": {
        "id": "qXCp7kbKtaZM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Rosembrok(X):#X es un vector.\n",
        "    return (np.sum(100*(X[1:]-X[:-1]**2)**2+(1-X[:-1])**2))#+10**(-3)*np.random.uniform(0,1))#Ruido\n",
        "def Rosembrok_r(X):#X es un vector.\n",
        "    return (np.sum(100*(X[1:]-X[:-1]**2)**2+(1-X[:-1])**2)+10**(-3)*np.random.uniform(0,1))#Ruido\n",
        "def Rosembrok_grad(X):\n",
        "    return scipy.optimize.approx_fprime(f=Rosembrok,xk=X,epsilon=10**(-4))\n",
        "def Rosembrok_grad_real(X):\n",
        "    D=[]\n",
        "    D.append(-400*(X[0]*(X[1]-X[0]**2))-2*(1-X[0]))\n",
        "    for i in range(1,len(X)-1):\n",
        "        D.append(-400*(X[i]*(X[i+1]-X[i]**2))+200*(X[i]-X[i-1]**2)-2*(1-X[i]))\n",
        "    D.append(200*(X[-1]-X[-2]**2))\n",
        "    return D\n",
        "def Rosembrok_Hessian(X):\n",
        "    M=[]\n",
        "    #Renglon 1\n",
        "    D=np.zeros(len(X))\n",
        "    D[0]=(-400*(X[1]-X[0]**2)+800*(X[0]**2)+2)\n",
        "    D[1]=-400*(X[1])\n",
        "    M.append(D)\n",
        "    #Renglon 2<i<n\n",
        "    for i in range(1,len(X)-1):\n",
        "        D=np.zeros(len(X))\n",
        "        D[i]=(-400*(X[i+1]-X[i]**2)+800*(X[i]**2)+202)\n",
        "        D[i-1]=-400*X[i-1]\n",
        "        D[i+1]=-400*X[i]\n",
        "        M.append(D)\n",
        "    #Renglon n\n",
        "    D=np.zeros(len(X))\n",
        "    D[-2]=-400*X[-2]\n",
        "    D[-1]=200\n",
        "    M.append(D)\n",
        "    return M"
      ],
      "metadata": {
        "id": "nLW9VeQvceE5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Muestra de la estimacion del gradiente con Forward difference.\n",
        "n=10\n",
        "x_k=np.random.uniform(0,1,size=n)\n",
        "p=np.eye(1,len(x_k),np.random.randint(0,len(x_k)))\n",
        "e_f,h=rutina_h(f=Rosembrok,p=p,m=6,x_0=x_k)\n",
        "print(h)\n",
        "print(Rosembrok_grad(x_k))\n",
        "print(Rosembrok_grad_real(x_k))\n",
        "#------------------------------------------------------"
      ],
      "metadata": {
        "id": "SRlkKtn12meB",
        "outputId": "cb0f46a6-e25f-4f86-a6f0-831a2943aecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No hay regla de decision\n",
            "5.693217339611057e-06\n",
            "[-131.03915073  294.0360645  -161.9710752    85.21104486 -102.82548412\n",
            "  196.78480104   99.30231516  -76.7683172   -19.57062441  136.0000247 ]\n",
            "[-136.9864118591995, 295.2122906438875, -163.38006442845904, 77.59144469558272, -110.91421588751201, 195.8608453201739, 92.3540666174858, -77.81330782420555, -18.26947814892475, 132.4779006276911]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relaxed_Armijo(x_k,f,fprime,d,e_f=0.0,c_1=0.1,c_2=0.9,\n",
        "                   max_iter=10**2):##Intenté usar scipy.line_search(), pero Strong Wolfe son dificiles de satisfacerse\n",
        "    #Metodo visto en clase.\n",
        "    iter=0\n",
        "    alpha_0=1\n",
        "    alpha=0\n",
        "    grad_f_k=fprime(x_k)\n",
        "    f_k=f(x_k)\n",
        "    beta=np.inf\n",
        "    alpha_i=alpha_0\n",
        "    alpha_armijo=0\n",
        "    g_dot_d=np.dot(grad_f_k,d)\n",
        "    for i in range(max_iter):\n",
        "        if(f(x_k+alpha_i*d)>f_k+c_1*alpha_i*g_dot_d+2*e_f):#Aqui esta la condicion relajada, dado e_f estimacion del error estandar.\n",
        "            beta=alpha_i\n",
        "            alpha_i=0.5*(alpha+beta)\n",
        "        else:\n",
        "            if(np.dot(fprime(x_k+alpha_i*d),d)<c_2*np.dot(grad_f_k,d)):\n",
        "                alpha=alpha_i\n",
        "                if(beta==np.inf):\n",
        "                    alpha_i=2*alpha\n",
        "                else:\n",
        "                    alpha_k=0.5*(alpha+beta)\n",
        "            else:\n",
        "                break\n",
        "    return alpha_i"
      ],
      "metadata": {
        "id": "U2R6IwNuNpFq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recovery(x_k,f,grad_k,e_f,h,l1,l2,d_k,m=6):#Genera un nuevo h\n",
        "    e_f_b,h_b=rutina_h(f=f,p=d_k,m=m,x_0=x_k)\n",
        "    n=len(d_k)\n",
        "    f_k=f(x_k)\n",
        "    if(h_b<l1*h or h_b>l2*h):\n",
        "        return e_f_b,h_b\n",
        "    else:\n",
        "        d_k=d_k/np.linalg.norm(d_k)\n",
        "        x_h=x_k+h*(d_k)\n",
        "        f_h=f(x_h)\n",
        "        if(f_h<=f(x_k)+0.001*h*np.dot(grad_k,d_k)+2*e_f):\n",
        "            return e_f,h\n",
        "        else:\n",
        "            f_stencil=[f(x_k+h*np.eye(1,n,i)) for i in range(n)]\n",
        "            f_s=np.min(f_stencil)\n",
        "            if(f_h<=f_s and f_h<=f_k):\n",
        "                return e_f,h\n",
        "            else:\n",
        "                if(f_k>f_s and f_h>f_s):\n",
        "                    return e_f,h\n",
        "                else:\n",
        "                    e_f_n,h_n=rutina_h(f=f,p=np.random.uniform(-1,1,size=n),m=m,x_0=x_k)\n",
        "                    return e_f_n,h_n"
      ],
      "metadata": {
        "id": "kcexZsIE9aMQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimum(f,x_0,tol_g=10**(-4),max_iter=10**2):\n",
        "    t1 = time.time()\n",
        "    #----------------------\n",
        "    #Listas\n",
        "    iter=0\n",
        "    GG=[]\n",
        "    Y=[]\n",
        "    S=[]\n",
        "    #----------------------\n",
        "    #----------------------\n",
        "    g_0=Rosembrok_grad(x_0)#Gradiente REAL\n",
        "    alpha_k=relaxed_Armijo(f=f,fprime=Rosembrok_grad,x_k=x_0,d=-g_0,e_f=0,max_iter=20)#line_search\n",
        "    x_1=x_0-alpha_k*g_0#Primera direccion de descenso es -grad(x_0)\n",
        "    g_1=Rosembrok_grad(x_1)\n",
        "    GG.append(np.linalg.norm(g_1))\n",
        "    #e_f,h=rutina_h(f=f,p=np.random.uniform(-1,1,size=n),m=6,x_0=x_0)\n",
        "    #----------------------\n",
        "    #diferencia de gradientes\n",
        "    #diferencia de x's\n",
        "    Y.append((g_1-g_0))\n",
        "    S.append(-alpha_k*g_0)\n",
        "    #comp=[np.linalg.norm(d_k_r+g_0)]#Lista para compararlos\n",
        "    #H_k=(np.dot(s_k,y_k)/(np.dot(y_k,y_k)))*np.identity(n)#Dado en clase.\n",
        "    #----------------------\n",
        "    while(np.linalg.norm(g_1)>tol_g and iter<max_iter):\n",
        "        x_0=x_1\n",
        "        g_0=g_1\n",
        "        #H_k_m=BFGS(s_k=s_k,y_k=y_k,g_k=0,H_k=H_k)\n",
        "        #d_k_r=-np.dot(np.linalg.inv(Rosembrok_Hessian(x_0)),g_0)\n",
        "        d_k=-L_BFGS(S=S,Y=Y,g=g_1)\n",
        "        #d_k=-np.dot(H_k_m,g_0)\n",
        "        if(iter>100):\n",
        "            del S[0]\n",
        "            del Y[0]\n",
        "        alpha_k_try=relaxed_Armijo(f=f,fprime=Rosembrok_grad,x_k=x_0,d=d_k,e_f=0,max_iter=20)#line_search\n",
        "        if(alpha_k_try>0):\n",
        "            alpha_k=alpha_k_try\n",
        "        else:\n",
        "            e_f,h=recovery(x_k=x_0,f=f,grad_k=g_0,e_f=e_f,h=h,l1=0.4,l2=2.5,d_k=d_k)\n",
        "            alpha_k_try=relaxed_Armijo(f=f,fprime=Rosembrok_grad,x_k=x_0,d=d_k,e_f=0,max_iter=10**6)#line_search\n",
        "            if(alpha_k_try==0):\n",
        "                print(\"Y ahora que\")\n",
        "                break\n",
        "            else: \n",
        "                alpha_k=alpha_k_try\n",
        "        x_1=x_0+alpha_k*d_k#\n",
        "        g_1=Rosembrok_grad(x_1)\n",
        "        iter+=1\n",
        "        #H_k=H_k_m\n",
        "        Y.append(g_1-g_0)\n",
        "        S.append(alpha_k*d_k)\n",
        "        #print(np.linalg.norm(d_k-d_k_l))\n",
        "        GG.append(np.linalg.norm(g_1))\n",
        "    print(\"Summary: \")\n",
        "    \n",
        "    #print(\"comp:\",comp[-1])\n",
        "    #plt.plot(GG)\n",
        "    #plt.show()\n",
        "    t2 = time.time()\n",
        "    tim = t2-t1\n",
        "    niter = iter\n",
        "    print(\"time: {:.02e}\\t Niter: {} \\t ||g_k||: {:.02e} \".format(\n",
        "             tim,niter,GG[-1]))\n",
        "    return x_1"
      ],
      "metadata": {
        "id": "RvIgGawfE0Se"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "n=100\n",
        "x_0=np.random.uniform(0,2,size=n)\n",
        "x_opt=find_minimum(f=Rosembrok_r,x_0=x_0,tol_g=10**(-2),max_iter=1000)"
      ],
      "metadata": {
        "id": "n5k_pB5TAc4_",
        "outputId": "89cb9f99-65c7-4787-ab42-a8df43ba07d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: \n",
            "time: 2.52e+00\t Niter: 228 \t ||g_k||: 6.52e-03 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r9J2WxiJ_DJq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pk0WC1zOiMpU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wfzzkEc_iMf_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def find_minimum(f,x_0,m,a_max,zeta,tol_g,max_iter=10**2):\n",
        "    f_0=f(x_0)\n",
        "    n=len(x_0)\n",
        "    p=np.random.uniform(-1,1,size=n)#Direccion aleatoria\n",
        "    p=p/np.linalg.norm(p)\n",
        "    #Rutina_h-----------------\n",
        "    delta=10**(-3)#Suggested by ECnoise algorithm.\n",
        "    F=[f(x_0+delta*i*p) for i in range(m)]\n",
        "    T=ECNoise(F)\n",
        "    e_f=var_estimation(T)\n",
        "    print(\"e_f: \",e_f)#Breakpoint\n",
        "    mu=aprox_f_biprima(e_f=e_f,f=f,x_k=x_0,p=p)\n",
        "    h=(8**(1/4))*(e_f/mu)**(1/2)\n",
        "    print(\"h: \", h)\n",
        "    #-------------------\n",
        "    def fprime(x):\n",
        "        return scipy.optimize.approx_fprime(f=f,xk=x,epsilon=h**(1/3))\n",
        "    g_0=fprime(x_0)\n",
        "    Y=[]\n",
        "    S=[]\n",
        "    #print(g_0)\n",
        "    #Una iteracion\n",
        "    iter=1\n",
        "    alpha_k=relaxed_Wolfe(x_k=x_0,f=f,fprime=fprime,d=-g_0,e_f=e_f,c_1=0.001,c_2=0.9,max_iter=10**4)\n",
        "    x_1=x_0-alpha_k*g_0#Actualizar x_k+1\n",
        "    f_k_m=f(x_1)\n",
        "    g_1=fprime(x_1)\n",
        "    Y.append(g_1-g_0)#diferencia de gradientes\n",
        "    S.append(-alpha_k*g_0)#diferencia de x's\n",
        "    GG=[]#Para guardar las normas de los gradientes y ver el progreso.\n",
        "    while(np.linalg.norm(g_1)>tol_g and iter<max_iter):\n",
        "        x_0=x_1\n",
        "        g_0=g_1\n",
        "        d_k=L_BFGS(S=S,Y=Y,g=g_1)#Calcula direccion de descenso.\n",
        "        alpha_k=relaxed_Wolfe(x_k=x_0,f=f,fprime=fprime,d=d_k,e_f=e_f,c_1=0.001,c_2=0.9,max_iter=10**4)#Tamaño de paso.\n",
        "        x_1=x_0+alpha_k*d_k\n",
        "        f_k_m=f(x_1)\n",
        "        g_1=fprime(x_1)\n",
        "        Y.append(g_1-g_0)\n",
        "        S.append(x_1-x_0)\n",
        "        iter+=1\n",
        "        GG.append(np.linalg.norm(x_1))\n",
        "    plt.plot(GG)\n",
        "    print(\"norma del gradiente final: \",GG[-2])##Antes del error si es que falla.\n",
        "    return x_1"
      ],
      "metadata": {
        "id": "ggtkNN6tbY0F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eEfbvYVc-2f_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test--------------------\n",
        "#x_0=np.random.uniform(0,1,size=100)\n",
        "#w=find_minimum(f=Rosembrok,x_0=w,m=6,a_max=100,zeta=0.5,tol_g=10**(-2),max_iter=10)\n",
        "#---------------------"
      ],
      "metadata": {
        "id": "e_mMgSLoehAK"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}