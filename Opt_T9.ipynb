{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled107.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNhja0tUCpouVfnmSd30jHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JairEsc/Mat_Apl/blob/main/Opt_T9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FR: $\\beta_{k+1}^{FR}=\\frac{||\\nabla f(x_{k+1})||^2}{||\\nabla f(x_{k})||^2}$\n",
        "\n",
        "PR: $\\beta_{k+1}^{PR}=\\frac{\\nabla^T f(x_{k+1})\\cdot( \\nabla f(x_{k+1})-\\nabla f(x_k))}{||\\nabla f(x_{k})||^2}$\n",
        "\n",
        "FR-PR: $\\beta_{k+1}^{FR-PR}=\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t-\\beta^{FR}_{k+1}, \\text{ if } \\beta_{k+1}^{PR}\\leq -\\beta_{k+1}^{FR}\\\\\n",
        "\t\t\\beta^{PR}_{k+1}, \\text{ if } |\\beta_{k+1}^{PR}|\\leq \\beta_{k+1}^{FR}\\\\\n",
        "\t\t\\beta^{FR}_{k+1}, \\text{ if } \\beta_{k+1}^{PR}> \\beta_{k+1}^{FR}\\\\\n",
        "\t\\end{array}\n",
        "\\right.$\n",
        "\n",
        "HS $\\beta_{k+1}^{HS}=\\frac{\\nabla^T f(x_{k+1})\\cdot( \\nabla f(x_{k+1})-\\nabla f(x_k)}{ (\\nabla f(x_{k+1})-\\nabla f(x_k))^T\\cdot d_k}$"
      ],
      "metadata": {
        "id": "8yQn3_KVXZNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General\n",
        "Algorithm:\n",
        "$$\n",
        "d_0=-\\nabla f(x_0), k=0$$\n",
        "$$\n",
        "while(||d_k||>tol_g):$$\n",
        "$$\\text{Compute } \\alpha_k \\text{ using line search algorithm}$$\n",
        "$$\\text{Update } x_{k-1}=x_k+\\alpha_kd_k$$\n",
        "$$\\text{Compute } \\nabla f(x_{k+1})$$\n",
        "$$\\text{Choose } \\beta_{k+1} \\text{ using a method.}$$\n",
        "$$\\text{Update } d_{k+1}=-\\nabla f(x_{k+1})+\\beta_{k}\\cdot d_k$$\n",
        "$$k=k+1$$\n"
      ],
      "metadata": {
        "id": "p-cxTFk6SdWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import optimize\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "D6jt0sPlm6Yo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "TvZ9xaDAPgZR"
      },
      "outputs": [],
      "source": [
        "#Gradiente conjugado con varios metodos---------------------------------\n",
        "def choose_beta(grad_f_kp1,grad_f_k,d_k,method):\n",
        "    if (method=='FR'):\n",
        "        return (np.dot(grad_f_kp1,grad_f_kp1-np.array(grad_f_kp1)))/(np.dot(grad_f_k,grad_f_k))\n",
        "    if (method=='PR'):\n",
        "        return (np.dot(grad_f_kp1,grad_f_kp1))/(np.dot(grad_f_k,grad_f_k))\n",
        "    if (method=='HS'):\n",
        "        return (np.dot(grad_f_kp1,grad_f_kp1-np.array(grad_f_kp1)))/(np.dot(grad_f_kp1-grad_f_k,d_k))\n",
        "    if (method=='FR-PR'):\n",
        "        bFR=(np.dot(grad_f_kp1,grad_f_kp1-np.array(grad_f_kp1)))/(np.dot(grad_f_k,grad_f_k))\n",
        "        bPR=(np.dot(grad_f_kp1,grad_f_kp1))/(np.dot(grad_f_k,grad_f_k))\n",
        "        if(abs(bPR)<=bFR):\n",
        "            return bPR\n",
        "        else:\n",
        "            if(bPR<-bFR):\n",
        "                return -bFR\n",
        "            else:\n",
        "                return bFR\n",
        "\n",
        "def GC(f,grad_f,x_0,tol_g=10**(-4),max_iter=10**3,method='FR'):\n",
        "    d_0=-1.*np.array(grad_f(x_0))\n",
        "    g_k=-d_0\n",
        "    k=0\n",
        "    while(np.linalg.norm(d_0)>=tol_g and k<max_iter):\n",
        "        alpha_k_gc=optimize.line_search(f=f,myfprime=grad_f,xk=x_0,pk=d_0,c1=0.001,c2=0.9)[0] #Tam. de paso\n",
        "        x_1=x_0+alpha_k_gc*d_0 #update\n",
        "        g_km1=grad_f(x_1) \n",
        "        beta_k_gc=choose_beta(grad_f_kp1=g_km1,grad_f_k=g_k,d_k=d_0,method=method)\n",
        "        d_0=-np.array(g_km1)+beta_k_gc*d_0\n",
        "        x_0=x_1\n",
        "        k+=1\n",
        "    print(\"iter\",k)\n",
        "    return x_0\n",
        "#-----------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lamb=100\n",
        "G=[]\n",
        "mu=0.01\n",
        "def fun_obj(X):#X es una matriz de nxn\n",
        "    f=0\n",
        "    for i in range(X.shape(0)):\n",
        "        for j in range(X.shape(1)):\n",
        "            f+=(X[i,j]-G[i,j])^2+lamb*(np.sqrt((X[i,j]-X[i+1,j])**2+mu)+np.sqrt((X[i,j]-X[i-1,j])**2+mu)+np.sqrt((X[i,j]-X[i,j+1])**2+mu)+np.sqrt((X[i,j]-X[i,j-1])**2+mu))\n",
        "    return f"
      ],
      "metadata": {
        "id": "ERNCH7iutj8k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}